{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d824edc9",
   "metadata": {},
   "source": [
    "1. What are Sequence-to-sequence models?\n",
    "2. What are the Problem with Vanilla RNNs?\n",
    "3. What is Gradient clipping?\n",
    "4. Explain Attention mechanism\n",
    "5. Explain Conditional random fields (CRFs)\n",
    "6. Explain self-attention\n",
    "7. What is Bahdanau Attention?\n",
    "8. What is a Language Model?\n",
    "9. What is Multi-Head Attention?\n",
    "10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd47a89",
   "metadata": {},
   "source": [
    "# answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12d7d1",
   "metadata": {},
   "source": [
    "1. Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture that can process a variable-length input sequence and generate a corresponding variable-length output sequence. They are commonly used for tasks such as machine translation, speech recognition, and text summarization.\n",
    "2. The main problem with Vanilla RNNs is the vanishing and exploding gradient problem, which can occur when gradients are propagated through many time steps, causing the gradients to become very small or very large and making training difficult.\n",
    "3. Gradient clipping is a technique used to prevent the gradients from becoming too large during training. It involves scaling the gradients down if they exceed a certain threshold, which helps to prevent the exploding gradient problem.\n",
    "4. Attention is a mechanism used in Seq2Seq models that allows the model to focus on specific parts of the input sequence when generating the output sequence. It works by assigning a weight to each input element based on its relevance to the current output element, and then taking a weighted sum of the input elements to compute the context vector for the current output element.\n",
    "5. Conditional random fields (CRFs) are a type of probabilistic graphical model used for sequence labeling tasks, such as part-of-speech tagging and named entity recognition. They model the conditional distribution of the output sequence given the input sequence, and use the Viterbi algorithm to find the most likely output sequence.\n",
    "6. Self-attention is an attention mechanism that allows a model to relate different parts of the input sequence to each other when computing the attention weights. It is used in models such as the Transformer to improve performance on tasks such as machine translation and language modeling.\n",
    "7. Bahdanau Attention is a specific type of attention mechanism that was introduced in 2014 for Seq2Seq models. It uses a feedforward neural network to compute the attention weights based on the current output element and the previous hidden state of the decoder, and has been shown to improve performance on machine translation tasks.\n",
    "8. A Language Model is a type of model that can predict the probability of a sequence of words or characters. It is typically trained on a large corpus of text data and can be used for tasks such as text generation, machine translation, and speech recognition.\n",
    "9. Multi-Head Attention is a variant of the attention mechanism used in the Transformer model, where multiple attention heads are used to compute different attention weights for the input sequence. This allows the model to capture more complex relationships between the input and output sequences and has been shown to improve performance on machine translation tasks.\n",
    "10. Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine translations. It compares the output of a machine translation system to one or more human translations, and assigns a score based on the n-gram overlap between the two. Higher scores indicate better translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d860b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
