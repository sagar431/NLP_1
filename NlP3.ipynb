{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cf1e66",
   "metadata": {},
   "source": [
    "1. Explain the basic architecture of RNN cell.\n",
    "2. Explain Backpropagation through time (BPTT)\n",
    "3. Explain Vanishing and exploding gradients\n",
    "4. Explain Long short-term memory (LSTM)\n",
    "5. Explain Gated recurrent unit (GRU)\n",
    "6. Explain Peephole LSTM\n",
    "7. Bidirectional RNNs\n",
    "8. Explain the gates of LSTM with equations.\n",
    "9. Explain BiLSTM\n",
    "10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2205de66",
   "metadata": {},
   "source": [
    "# answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb0102",
   "metadata": {},
   "source": [
    "1. The basic architecture of an RNN cell consists of an input layer, a hidden layer, and an output layer. The input layer takes in the input at each time step, and the hidden layer updates its state based on the input and the previous hidden state. The output layer produces an output based on the hidden state at each time step. The hidden layer contains a set of recurrent weights that allow information to be propagated across time steps.\n",
    "\n",
    "2. Backpropagation through time (BPTT) is a method for training RNNs that involves unrolling the network over time and applying backpropagation to calculate the gradients of the loss function with respect to the parameters. BPTT involves calculating the gradients at each time step and then summing them up over all time steps to update the parameters.\n",
    "\n",
    "3. Vanishing and exploding gradients are problems that can occur in RNNs when the gradients become very small or very large, respectively, during backpropagation. This can happen when the weights are updated repeatedly over many time steps, leading to exponential growth or decay of the gradients. Vanishing gradients can make it difficult for the network to learn long-term dependencies, while exploding gradients can cause the network to diverge.\n",
    "\n",
    "4. Long short-term memory (LSTM) is a type of RNN architecture that uses memory cells to selectively remember or forget information over time. LSTMs have gates that control the flow of information into and out of the memory cells, including an input gate, a forget gate, and an output gate. These gates allow the network to selectively update and remember relevant information over long time periods.\n",
    "\n",
    "5. Gated recurrent unit (GRU) is a type of RNN architecture that is similar to LSTM but has fewer gates. GRUs have a reset gate and an update gate that control the flow of information into and out of the hidden state. The reset gate determines how much of the previous hidden state to forget, while the update gate determines how much of the new input to add to the hidden state.\n",
    "\n",
    "6. Peephole LSTM is an extension of LSTM that adds additional connections from the memory cell to the gates. These connections allow the gates to directly access the memory cell, enabling the network to better remember or forget information based on the current state of the cell.\n",
    "\n",
    "7. Bidirectional RNNs are a type of RNN architecture that process the input sequence in both forward and backward directions. This allows the network to take into account both past and future context when making predictions, and can improve the performance of tasks such as speech recognition and machine translation.\n",
    "\n",
    "8. The gates of LSTM are the input gate, the forget gate, and the output gate. The input gate controls how much of the new input to add to the memory cell, the forget gate controls how much of the previous memory cell to forget, and the output gate controls how much of the memory cell to output. These gates are controlled by sigmoid activation functions, and their equations are:\n",
    "\n",
    "- Input gate: i_t = σ(W_i x_t + U_i h_{t-1} + b_i)\n",
    "- Forget gate: f_t = σ(W_f x_t + U_f h_{t-1} + b_f)\n",
    "- Output gate: o_t = σ(W_o x_t + U_o h_{t-1} + b_o)\n",
    "\n",
    "9. BiLSTM is a type of RNN architecture that uses two LSTM networks, one processing the input sequence in forward direction and the other in backward direction. The output of each network is concatenated to form the final output, which takes into account both past and future context.\n",
    "\n",
    "10. BiGRU is a type of RNN architecture that uses two GRU networks, one processing the input sequence in forward direction and the other in backward direction. The output of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2e80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
