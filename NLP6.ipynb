{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcfb6fbf",
   "metadata": {},
   "source": [
    "1. What are Vanilla autoencoders\n",
    "2. What are Sparse autoencoders\n",
    "3. What are Denoising autoencoders\n",
    "4. What are Convolutional autoencoders\n",
    "5. What are Stacked autoencoders\n",
    "6. Explain how to generate sentences using LSTM autoencoders\n",
    "7. Explain Extractive summarization\n",
    "8. Explain Abstractive summarization\n",
    "9. Explain Beam search\n",
    "10. Explain Length normalization\n",
    "11. Explain Coverage normalization\n",
    "12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8993371",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d0edb",
   "metadata": {},
   "source": [
    "1. Vanilla autoencoders are a type of neural network architecture used for unsupervised learning. They consist of an encoder network that takes input data and compresses it into a low-dimensional latent space, and a decoder network that takes the compressed data and reconstructs it back to the original input space.\n",
    "\n",
    "2. Sparse autoencoders are similar to vanilla autoencoders, but with a regularization technique that encourages the encoder to produce sparse representations. This means that only a small subset of the latent features are activated for any given input, leading to more efficient encoding.\n",
    "\n",
    "3. Denoising autoencoders are trained to reconstruct data that has been corrupted with noise. By introducing noise into the input and training the network to reconstruct the original clean data, denoising autoencoders learn robust feature representations that are useful for denoising.\n",
    "\n",
    "4. Convolutional autoencoders use convolutional layers in both the encoder and decoder networks, making them suitable for processing structured input data like images.\n",
    "\n",
    "5. Stacked autoencoders consist of multiple layers of encoders and decoders, allowing for more complex feature representations and improved reconstruction performance.\n",
    "\n",
    "6. To generate sentences using LSTM autoencoders, a decoder LSTM is trained on the encoded representations produced by the encoder LSTM. The decoder generates one word at a time, conditioned on the previous words and the encoded representation.\n",
    "\n",
    "7. Extractive summarization is a type of summarization that selects important sentences or phrases from the original text to create a summary.\n",
    "\n",
    "8. Abstractive summarization is a type of summarization that generates a summary by paraphrasing and condensing the content of the original text, often using natural language generation techniques.\n",
    "\n",
    "9. Beam search is a search algorithm used in natural language processing tasks like machine translation and text generation to generate the most likely sequence of words. It considers multiple possible output sequences at each step and selects the most promising candidates based on a score.\n",
    "\n",
    "10. Length normalization is a technique used in natural language processing to adjust the scores of generated text based on the length of the output. Longer sequences tend to have lower probabilities, so length normalization adjusts the scores to account for this.\n",
    "\n",
    "11. Coverage normalization is a technique used in neural machine translation to ensure that the generated text covers all of the information present in the input sequence. It penalizes sequences that generate duplicate or incomplete information, and encourages the model to generate summaries that are comprehensive and accurate.\n",
    "\n",
    "12. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of text summarization and machine translation outputs. It compares the overlap between the generated text and the reference summaries or translations, using measures like F1-score and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335ad40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
