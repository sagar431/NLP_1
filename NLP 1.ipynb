{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125e5964",
   "metadata": {},
   "source": [
    "1. Explain One-Hot Encoding\n",
    "2. Explain Bag of Words\n",
    "3. Explain Bag of N-Grams\n",
    "4. Explain TF-IDF\n",
    "5. What is OOV problem?\n",
    "6. What are word embeddings?\n",
    "7. Explain Continuous bag of words (CBOW)\n",
    "8. Explain SkipGram\n",
    "9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efdb4b",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f1289",
   "metadata": {},
   "source": [
    "1. One-Hot Encoding is a process of representing categorical data as numerical data. It involves creating a binary vector of size n, where n is the number of categories in the data. Each vector element corresponds to a category, and only the element corresponding to the category of an observation is set to 1, while all other elements are set to 0.\n",
    "\n",
    "2. Bag of Words is a technique used for text classification and natural language processing. It involves representing a document as a collection of its words, disregarding their order and focusing on their frequency. The document is then represented as a sparse vector, where each element represents the frequency of a word in the document.\n",
    "\n",
    "3. Bag of N-Grams is a technique that extends the Bag of Words approach by also considering n-grams, which are contiguous sequences of n words in a document. The technique involves creating a sparse vector that represents the frequency of each word and n-gram in the document.\n",
    "\n",
    "4. TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a technique used to weigh the importance of words in a document based on their frequency in the document and their frequency in the corpus. The technique involves multiplying the frequency of a word in a document by the inverse of its frequency in the corpus.\n",
    "\n",
    "5. OOV stands for Out of Vocabulary problem. It refers to the issue of encountering words in a text that are not present in the vocabulary of a language model. This can lead to errors in text classification and natural language processing.\n",
    "\n",
    "6. Word embeddings are a way of representing words in a vector space, such that words with similar meanings are closer to each other in the space. They are learned by training a neural network to predict the probability of a word given its context or vice versa. The resulting word embeddings can be used to improve the performance of text classification and natural language processing models.\n",
    "\n",
    "7. Continuous bag of words (CBOW) is a type of neural network used for generating word embeddings. The model takes a window of surrounding words as input and predicts the target word in the center of the window. The weights of the network are used as word embeddings.\n",
    "\n",
    "8. SkipGram is another type of neural network used for generating word embeddings. It takes a target word as input and predicts the surrounding words in a window around the target word. The weights of the network are used as word embeddings.\n",
    "\n",
    "9. GloVe embeddings (Global Vectors for Word Representation) is a method for generating word embeddings that uses matrix factorization to capture the global co-occurrence statistics of words in a corpus. The resulting embeddings are weighted averages of the vector representations of each word, with the weights determined by the co-occurrence probabilities of each word pair. GloVe embeddings have been shown to outperform other methods on a range of natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96648ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
