{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38829831",
   "metadata": {},
   "source": [
    "1. Explain the architecture of BERT\n",
    "2. Explain Masked Language Modeling (MLM)\n",
    "3. Explain Next Sentence Prediction (NSP)\n",
    "4. What is Matthews evaluation?\n",
    "5. What is Matthews Correlation Coefficient (MCC)?\n",
    "6. Explain Semantic Role Labeling\n",
    "7. Why Fine-tuning a BERT model takes less time than pretraining\n",
    "8. Recognizing Textual Entailment (RTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308885c6",
   "metadata": {},
   "source": [
    "# answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294501bb",
   "metadata": {},
   "source": [
    "1. BERT, or Bidirectional Encoder Representations from Transformers, is a transformer-based language model developed by Google. The architecture consists of a multi-layer bidirectional transformer encoder that learns contextual relations between words in a sentence or sequence. BERT is pre-trained on a large corpus of text, and fine-tuned on specific tasks such as natural language processing tasks including sentiment analysis, named entity recognition, and question answering.\n",
    "\n",
    "2. Masked Language Modeling (MLM) is a pre-training technique used in BERT. In MLM, a certain percentage of the input tokens are randomly masked and the model is trained to predict the masked tokens based on the surrounding tokens.\n",
    "\n",
    "3. Next Sentence Prediction (NSP) is another pre-training technique used in BERT. NSP is used to train the model to understand the relationship between two sentences. The model is given two sentences as input and is trained to predict whether the second sentence is a plausible continuation of the first sentence.\n",
    "\n",
    "4. Matthews evaluation is a binary classification evaluation metric that measures the quality of predictions made by a classifier. It is defined as the correlation between the predicted and actual binary classifications and is considered a balanced metric even if the classes are of very different sizes.\n",
    "\n",
    "5. Matthews Correlation Coefficient (MCC) is a metric used to evaluate binary classification models. It takes into account true positives, true negatives, false positives, and false negatives, and produces a score between -1 and 1, where a score of 1 indicates a perfect prediction, 0 is no better than random, and -1 indicates total disagreement between prediction and actual values.\n",
    "\n",
    "6. Semantic Role Labeling is a natural language processing task where the goal is to identify the predicate-argument structure of a sentence. In other words, SRL aims to identify the role played by each noun phrase in the sentence with respect to the predicate.\n",
    "\n",
    "7. Fine-tuning a BERT model takes less time than pretraining because the pre-training phase requires a large amount of data and computational resources. Pre-training involves training the model on a large corpus of text to learn general language representations. Fine-tuning involves adapting the pre-trained model to a specific task, which requires much less data and resources.\n",
    "\n",
    "8. Recognizing Textual Entailment (RTE) is a natural language processing task where the goal is to determine whether a given piece of text (the hypothesis) is entailed by a given text (the premise). In other words, the task involves determining whether the hypothesis can be inferred from the premise. BERT can be fine-tuned for RTE and has shown state-of-the-art performance on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984eb9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
